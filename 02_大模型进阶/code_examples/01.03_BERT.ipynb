{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT模型详解\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) 是由Google AI研究团队于2018年提出的预训练语言模型，它彻底改变了自然语言处理领域。\n",
    "\n",
    "## BERT的核心特点\n",
    "\n",
    "1. **双向上下文编码**：与GPT等单向模型不同，BERT能够同时考虑文本的左右上下文，捕获更全面的语义信息\n",
    "2. **基于Transformer编码器**：BERT仅使用Transformer的编码器部分，不包含解码器\n",
    "3. **预训练-微调范式**：先在大规模无标注文本上预训练，再在下游任务上微调\n",
    "\n",
    "## 预训练任务\n",
    "\n",
    "BERT通过两个创新的预训练任务学习语言表示：\n",
    "\n",
    "1. **掩码语言模型(MLM)**：随机遮盖输入中15%的词元，训练模型预测这些被遮盖的词\n",
    "2. **下一句预测(NSP)**：训练模型判断两个句子是否为连续的句子对\n",
    "\n",
    "## 模型规模\n",
    "\n",
    "BERT有两种主要规格：\n",
    "- **BERT-Base**：12层Transformer编码器，768维隐藏层，12个注意力头，1.1亿参数\n",
    "- **BERT-Large**：24层Transformer编码器，1024维隐藏层，16个注意力头，3.4亿参数\n",
    "\n",
    "## 应用场景\n",
    "\n",
    "BERT在众多NLP任务中表现出色：\n",
    "- 文本分类\n",
    "- 命名实体识别\n",
    "- 问答系统\n",
    "- 情感分析\n",
    "- 文本相似度计算\n",
    "\n",
    "## BERT的影响与发展\n",
    "\n",
    "BERT开创了NLP预训练模型的新时代，催生了众多后续模型：\n",
    "- RoBERTa：优化了BERT的训练方法\n",
    "- DistilBERT：轻量化版本，保持性能的同时减少参数量\n",
    "- ALBERT：参数共享技术，减少内存消耗\n",
    "- ELECTRA：使用判别式预训练方法\n",
    "\n",
    "BERT的出现标志着NLP领域从特定任务模型向通用预训练模型的重要转变，为后来的大语言模型奠定了基础。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 导入BERT相关代码\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     LayerNorm,  \u001b[38;5;66;03m# 层归一化\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     MultiHeadAttention,  \u001b[38;5;66;03m# 多头注意力\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     PositionwiseFeedForward,  \u001b[38;5;66;03m# 位置前馈网络\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     TransformerBlock,  \u001b[38;5;66;03m# Transformer块\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     BertEmbeddings,  \u001b[38;5;66;03m# BERT嵌入层\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     BERT,  \u001b[38;5;66;03m# BERT模型\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     BertForMaskedLM,  \u001b[38;5;66;03m# 掩码语言模型\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     BertForSequenceClassification,  \u001b[38;5;66;03m# 序列分类模型\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     BertConfig  \u001b[38;5;66;03m# BERT配置\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 创建一个小型BERT配置用于演示\u001b[39;00m\n\u001b[1;32m     20\u001b[0m config \u001b[38;5;241m=\u001b[39m BertConfig(\n\u001b[1;32m     21\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30522\u001b[39m,  \u001b[38;5;66;03m# 词汇表大小\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,   \u001b[38;5;66;03m# 隐藏层大小\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# 丢弃率\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/ai/llm/02_大模型进阶/code_examples/bert.py:335\u001b[0m\n\u001b[1;32m    332\u001b[0m config \u001b[38;5;241m=\u001b[39m BertConfig(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30522\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# 创建BERT模型\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m bert \u001b[38;5;241m=\u001b[39m \u001b[43mBERT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai/llm/02_大模型进阶/code_examples/bert.py:209\u001b[0m, in \u001b[0;36mBERT.__init__\u001b[0;34m(self, vocab_size, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size, max_position_embeddings, type_vocab_size, dropout_rate)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mvocab_size: 词汇表大小\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03mhidden_size: 隐藏层大小\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03mdropout_rate: 丢弃率\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28msuper\u001b[39m(BERT, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mBertEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 词嵌入层\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    211\u001b[0m     TransformerBlock(hidden_size, num_attention_heads, intermediate_size, dropout_rate) \u001b[38;5;66;03m# 变压器块\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hidden_layers)\n\u001b[1;32m    213\u001b[0m ])\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# 初始化权重\u001b[39;00m\n",
      "File \u001b[0;32m~/ai/llm/02_大模型进阶/code_examples/bert.py:161\u001b[0m, in \u001b[0;36mBertEmbeddings.__init__\u001b[0;34m(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout_rate)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28msuper\u001b[39m(BertEmbeddings, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 词嵌入层\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(max_position_embeddings, hidden_size) \u001b[38;5;66;03m# 位置嵌入层\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(type_vocab_size, hidden_size) \u001b[38;5;66;03m# 类型嵌入层\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/sparse.py:167\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m--> 167\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    168\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 导入BERT相关代码\n",
    "from bert import (\n",
    "    LayerNorm,  # 层归一化\n",
    "    MultiHeadAttention,  # 多头注意力\n",
    "    PositionwiseFeedForward,  # 位置前馈网络\n",
    "    TransformerBlock,  # Transformer块\n",
    "    BertEmbeddings,  # BERT嵌入层\n",
    "    BERT,  # BERT模型\n",
    "    BertForMaskedLM,  # 掩码语言模型\n",
    "    BertForSequenceClassification,  # 序列分类模型\n",
    "    BertConfig  # BERT配置\n",
    ")\n",
    "\n",
    "# 创建一个小型BERT配置用于演示\n",
    "config = BertConfig(\n",
    "    vocab_size=30522,  # 词汇表大小\n",
    "    hidden_size=768,   # 隐藏层大小\n",
    "    num_hidden_layers=12,  # Transformer层数\n",
    "    num_attention_heads=12,  # 注意力头数\n",
    "    intermediate_size=3072,  # 前馈网络中间层大小\n",
    "    max_position_embeddings=512,  # 最大位置编码\n",
    "    type_vocab_size=2,  # 句子类型数量\n",
    "    dropout_rate=0.1  # 丢弃率\n",
    ")\n",
    "\n",
    "# 创建BERT模型\n",
    "bert_model = BERT(\n",
    "    vocab_size=config.vocab_size,   # 词汇表大小\n",
    "    hidden_size=config.hidden_size, # 隐藏层大小\n",
    "    num_hidden_layers=config.num_hidden_layers, # Transformer层数\n",
    "    num_attention_heads=config.num_attention_heads, # 注意力头数\n",
    "    intermediate_size=config.intermediate_size, # 前馈网络中间层大小\n",
    "    max_position_embeddings=config.max_position_embeddings, # 最大位置编码\n",
    "    type_vocab_size=config.type_vocab_size, # 句子类型数量\n",
    "    dropout_rate=config.dropout_rate # 丢弃率\n",
    ")\n",
    "\n",
    "# 打印模型结构\n",
    "print(f\"BERT模型参数量: {sum(p.numel() for p in bert_model.parameters()):,}\")\n",
    "\n",
    "# 创建用于掩码语言模型的BERT\n",
    "mlm_model = BertForMaskedLM(bert_model, config.vocab_size, config.hidden_size)\n",
    "\n",
    "# 创建用于序列分类的BERT (例如情感分析，2个类别)\n",
    "cls_model = BertForSequenceClassification(bert_model, num_labels=2, hidden_size=config.hidden_size)\n",
    "\n",
    "# 演示输入\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
    "token_type_ids = torch.zeros_like(input_ids)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "# 获取BERT输出\n",
    "with torch.no_grad():\n",
    "    bert_output = bert_model(input_ids, token_type_ids, attention_mask)\n",
    "    mlm_output = mlm_model(input_ids, token_type_ids, attention_mask)\n",
    "    cls_output = cls_model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "print(f\"\\nBERT输出形状: {bert_output.shape}\")  # [batch_size, seq_length, hidden_size]\n",
    "print(f\"掩码语言模型输出形状: {mlm_output.shape}\")  # [batch_size, seq_length, vocab_size]\n",
    "print(f\"序列分类模型输出形状: {cls_output.shape}\")  # [batch_size, num_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

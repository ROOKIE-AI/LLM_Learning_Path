# 大模型进阶

本目录包含LLM学习路径的第二阶段：大模型进阶的相关内容。

## 目录内容

- [Transformer深度解析](./01_Transformer深度解析.md)
  - Attention机制
  - Self-Attention与Multi-Head Attention
  - Encoder-Decoder架构
- [主流大模型研究](./02_主流大模型研究.md)
  - GPT系列
  - LLaMA系列
  - 其他知名模型
- [Prompt Engineering](./03_Prompt_Engineering.md)
  - 提示工程方法论
  - 优化策略
  - 安全与攻防
- [模型优化技术](./04_模型优化技术.md)
  - 微调策略
  - RLHF技术
  - PPO算法应用
- [数据工程](./05_数据工程.md)
  - 数据集处理
  - 数据标注
  - 评测体系
- [模型压缩与优化](./06_模型压缩与优化.md)
  - 知识蒸馏
  - 量化技术
  - 推理加速

## 学习目标

完成本阶段学习后，您应该能够：

1. 深入理解Transformer架构及其工作原理
2. 掌握主流大语言模型的结构特点和发展历程
3. 熟练运用Prompt Engineering技术优化模型输出
4. 了解模型微调、RLHF等优化技术的原理和应用
5. 掌握数据工程的基本流程和方法
6. 了解模型压缩与优化的主要技术

## 推荐资源

### 论文
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

### 教程与博客
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
- [Hugging Face Transformers文档](https://huggingface.co/docs/transformers/index)

### 实践项目
- [Transformer实现](https://github.com/huggingface/transformers)
- [PEFT: Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft)
- [TRL: Transformer Reinforcement Learning](https://github.com/huggingface/trl)

## 学习路径

建议按照以下顺序学习本阶段内容：

1. 首先深入学习Transformer架构，这是所有大语言模型的基础
2. 然后了解主流大模型的发展历程和技术特点
3. 学习Prompt Engineering技术，掌握与大模型交互的方法
4. 深入研究模型优化技术，了解如何提升模型性能
5. 学习数据工程和模型压缩技术，为实际应用做准备

完成本阶段学习后，您可以进入[第三阶段：工程实践](../03_工程实践/README.md)的学习。 
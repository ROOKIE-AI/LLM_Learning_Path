{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4077.16s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4082.35s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb                                 0.6.3\n",
      "cryptography                             44.0.2\n",
      "duckduckgo_search                        7.5.3\n",
      "langchain                                0.3.21\n",
      "langchain-community                      0.3.20\n",
      "langchain-core                           0.3.47\n",
      "langchain-deepseek                       0.1.3\n",
      "langchain-openai                         0.3.9\n",
      "langchain-text-splitters                 0.3.7\n",
      "openai                                   1.68.2\n",
      "tiktoken                                 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from IPython.display import Markdown\n",
    "from setenv import APIKeyManager\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "# 设置API密钥\n",
    "key_manager = APIKeyManager().setup_api_key(env_names=[\"OPENAI_API_KEY\", \"OPENAI_API_BASE\"])\n",
    "\n",
    "# !pip install chromadb\n",
    "# !pip install langchain-deepseek\n",
    "\n",
    "# 查看Python版本\n",
    "!python -V\n",
    "# 查看安装的库\n",
    "!pip list | grep -E 'langchain|openai|llm|tiktoken|chromadb|cryptography|duck'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 记忆类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 BufferMemory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferMemory 是最简单的记忆类型，它将所有对话历史存储在列表中<br/>\n",
    "memory_key 参数指定在链的输入中使用哪个键来存储对话历史<br/>\n",
    "return_messages=True 表示返回消息对象列表而不是字符串<br/>\n",
    "这种记忆类型适合短对话，但对于长对话可能会占用过多内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58988/1975669923.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n",
      "/tmp/ipykernel_58988/1975669923.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 创建LLM\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"deepseek-chat\")\n",
    "\n",
    "# 创建缓冲记忆\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # 指定记忆在链输入中的键名\n",
    "    return_messages=True        # 返回消息对象而非字符串\n",
    ")\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],\n",
    "    template=\"聊天历史：{chat_history}\\n用户输入：{input}\\n回答：\"\n",
    ")\n",
    "\n",
    "# 在链中使用\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '你好，我是小明。',\n",
       " 'chat_history': [HumanMessage(content='你好，我是小明。', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='你好，小明！很高兴认识你～😊 今天有什么想聊的话题吗？或者有什么我可以帮你的呢？', additional_kwargs={}, response_metadata={})],\n",
       " 'text': '你好，小明！很高兴认识你～😊 今天有什么想聊的话题吗？或者有什么我可以帮你的呢？'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"你好，我是小明。\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '我是Rookie',\n",
       " 'chat_history': [HumanMessage(content='你好，我是小明。', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='你好，小明！很高兴认识你～😊 今天有什么想聊的话题吗？或者有什么我可以帮你的呢？', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='我是Rookie', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='你好，Rookie！看来你换了个新名字呢～✨ 之前的小明是昵称吗？还是现在想用Rookie这个名字来聊天呢？  \\n\\n无论怎样，都很高兴认识你！😄 今天想聊聊什么？游戏、生活，还是有什么特别感兴趣的话题？', additional_kwargs={}, response_metadata={})],\n",
       " 'text': '你好，Rookie！看来你换了个新名字呢～✨ 之前的小明是昵称吗？还是现在想用Rookie这个名字来聊天呢？  \\n\\n无论怎样，都很高兴认识你！😄 今天想聊聊什么？游戏、生活，还是有什么特别感兴趣的话题？'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"我是Rookie\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 VectorStoreMemory\n",
    "VectorStoreMemory 使用向量存储来保存对话历史，适合长对话和语义搜索<br/>\n",
    "它可以根据当前查询检索相关的历史对话，而不是简单地返回所有历史<br/>\n",
    "这种记忆类型特别适合需要从大量历史对话中检索相关信息的场景<br/>\n",
    "下面是一个简单的VectorStoreMemory示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings  \n",
    "from langchain_community.vectorstores import FAISS  \n",
    "import tempfile  \n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# 由于没有langchain_deepseek.embeddings模块，我们使用OpenAI嵌入\n",
    "# 但配置为使用deepseek模型（如果API支持）\n",
    "embeddings = OpenAIEmbeddings(  \n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"text-embedding-ada-002\"  # 这里使用OpenAI的嵌入模型，实际使用时可替换为支持的deepseek嵌入模型\n",
    ")  \n",
    "\n",
    "# 创建临时目录来存储FAISS索引  \n",
    "persist_directory = tempfile.mkdtemp()  \n",
    "\n",
    "# 使用FAISS创建向量存储  \n",
    "vectorstore = FAISS.from_texts(  \n",
    "    [\"初始化向量存储\"],   \n",
    "    embedding=embeddings  \n",
    ")  \n",
    "\n",
    "# 创建记忆组件  \n",
    "memory = VectorStoreRetrieverMemory(  \n",
    "    retriever=vectorstore.as_retriever(),  \n",
    "    memory_key=\"chat_history\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向记忆中添加一些示例对话\n",
    "# 注意：由于出现NotFoundError: Error code: 404错误，\n",
    "# 可能是向量存储配置有问题，我们需要重新初始化向量存储\n",
    "# 确保向量存储正确创建并持久化到磁盘\n",
    "\n",
    "# 创建一个本地持久化的向量存储\n",
    "\n",
    "\n",
    "# 现在尝试保存上下文\n",
    "memory.save_context(\n",
    "    {\"input\": \"我最喜欢的颜色是蓝色\"},\n",
    "    {\"output\": \"蓝色是一个很棒的颜色！它让人联想到海洋和天空。\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"我喜欢在周末去爬山\"},\n",
    "    {\"output\": \"爬山是一项很好的活动，既能锻炼身体又能亲近自然。\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"我养了一只叫小花的猫\"},\n",
    "    {\"output\": \"小花是个可爱的名字！猫咪是很好的宠物伴侣。\"}\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个友好的助手，能够记住用户之前提到的信息。\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"human\", \"相关的对话历史：\\n{chat_history}\")\n",
    "])\n",
    "\n",
    "# 创建链\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"deepseek-chat\")\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试向量存储记忆\n",
    "response = chain.invoke({\"input\": \"我之前说过我喜欢什么颜色？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

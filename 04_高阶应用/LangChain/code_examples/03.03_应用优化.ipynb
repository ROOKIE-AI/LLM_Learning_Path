{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åº”ç”¨ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.21\n",
      "chromadb                                 0.6.3\n",
      "cryptography                             44.0.2\n",
      "duckduckgo_search                        7.5.3\n",
      "langchain                                0.3.21\n",
      "langchain-community                      0.3.20\n",
      "langchain-core                           0.3.47\n",
      "langchain-deepseek                       0.1.3\n",
      "langchain-experimental                   0.3.4\n",
      "langchain-openai                         0.3.9\n",
      "langchain-text-splitters                 0.3.7\n",
      "numpy                                    2.0.2\n",
      "openai                                   1.68.2\n",
      "scipy                                    1.10.1\n",
      "tiktoken                                 0.9.0\n",
      "unstructured                             0.17.2\n",
      "unstructured-client                      0.31.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from IPython.display import Markdown\n",
    "from setenv import APIKeyManager\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "# è®¾ç½®APIå¯†é’¥\n",
    "key_manager = APIKeyManager().setup_api_key([\"DEEPSEEK_API_KEY\"])\n",
    "\n",
    "# !pip install langchain-experimental\n",
    "\n",
    "# æŸ¥çœ‹Pythonç‰ˆæœ¬\n",
    "!python -V\n",
    "# æŸ¥çœ‹å®‰è£…çš„åº“\n",
    "!pip list | grep -E 'langchain|openai|llm|tiktoken|chromadb|cryptography|duck|unstructured|numpy|scipy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ€§èƒ½ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 å“åº”ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼‚æ­¥å’Œæ‰¹é‡å¤„ç†:\n",
    "\n",
    "åœ¨å¤„ç†å¤§é‡è¯·æ±‚æ—¶ï¼Œå¼‚æ­¥å’Œæ‰¹é‡å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼š\n",
    "\n",
    "1. **å¼‚æ­¥å¤„ç†**ï¼šä½¿ç”¨`async/await`è¯­æ³•å®ç°éé˜»å¡æ“ä½œï¼Œè®©ç¨‹åºåœ¨ç­‰å¾…APIå“åº”æ—¶å¯ä»¥æ‰§è¡Œå…¶ä»–ä»»åŠ¡\n",
    "2. **æ‰¹é‡å¤„ç†**ï¼šåŒæ—¶å‘é€å¤šä¸ªè¯·æ±‚ï¼Œå¹¶è¡Œå¤„ç†å¤šä¸ªä»»åŠ¡ \n",
    "ä¸‹é¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨LangChainçš„å¼‚æ­¥APIå’Œå›è°ƒå‡½æ•°æ¥ä¼˜åŒ–å“åº”æ€§èƒ½\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek  # å¯¼å…¥DeepSeekèŠå¤©æ¨¡å‹\n",
    "import asyncio  # å¯¼å…¥å¼‚æ­¥IOåº“ï¼Œç”¨äºå®ç°å¹¶å‘å¤„ç†\n",
    "\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== å•ä¸ªå¼‚æ­¥ç”Ÿæˆç¤ºä¾‹ =====\n",
      "ç”Ÿæˆç»“æœ: ä½ å¥½ï¼ğŸ˜Š å¾ˆé«˜å…´è§åˆ°ä½ ï½æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿ\n",
      "\n",
      "===== æ‰¹é‡å¤„ç†ç¤ºä¾‹ =====\n",
      "æç¤º 1 çš„ç»“æœ: Python æ˜¯ä¸€ç§é«˜çº§ã€è§£é‡Šå‹ã€é€šç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œç”± **Guido van Rossum** äº ...\n",
      "\n",
      "æç¤º 2 çš„ç»“æœ: äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§° AIï¼‰æ˜¯æŒ‡ç”±è®¡ç®—æœºç³»ç»Ÿæ¨¡æ‹Ÿã€å»¶ä¼¸æˆ–æ‰©...\n",
      "\n",
      "æç¤º 3 çš„ç»“æœ: å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰æ˜¯ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNL...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æ¼”ç¤ºå®Œæˆ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ¼”ç¤ºå¼‚æ­¥ç”Ÿæˆå’Œæ‰¹é‡å¤„ç†\n",
    "async def demo_async_and_batch():\n",
    "    print(\"===== å•ä¸ªå¼‚æ­¥ç”Ÿæˆç¤ºä¾‹ =====\")\n",
    "    response = await llm.agenerate([\"ä½ å¥½\"])\n",
    "    print(f\"ç”Ÿæˆç»“æœ: {response.generations[0][0].text}\\n\")\n",
    "    \n",
    "    print(\"===== æ‰¹é‡å¤„ç†ç¤ºä¾‹ =====\")\n",
    "    prompts = [\"ä»‹ç»ä¸€ä¸‹Python\", \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\", \"è§£é‡Šä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹\"]\n",
    "    # ä¿®æ”¹æ‰¹é‡å¤„ç†å‡½æ•°è°ƒç”¨\n",
    "    tasks = [llm.agenerate([prompt]) for prompt in prompts] # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡åˆ—è¡¨\n",
    "    responses = await asyncio.gather(*tasks) # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œæé«˜å¤„ç†æ•ˆç‡\n",
    "    \n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"æç¤º {i+1} çš„ç»“æœ: {response.generations[0][0].text[:50]}...\\n\")\n",
    "    \n",
    "    return \"æ¼”ç¤ºå®Œæˆ\"\n",
    "\n",
    "# æ‰§è¡Œæ¼”ç¤º\n",
    "await demo_async_and_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ç¼“å­˜ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache  \n",
    "from langchain.cache import InMemoryCache  \n",
    "\n",
    "# å…¨å±€è®¾ç½®ç¼“å­˜  \n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ç¼“å­˜ä¼˜åŒ–æ¼”ç¤º =====\n",
      "ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰:\n",
      "- è€—æ—¶: 45.26ç§’\n",
      "- ç»“æœå‰50ä¸ªå­—ç¬¦: é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦åŸç†ï¼ˆå¦‚å åŠ æ€å’Œçº ç¼ æ€ï¼‰è¿›è¡Œä¿¡æ¯å¤„ç†çš„æ–°å‹è®¡ç®—èŒƒå¼ã€‚å…¶æ ¸å¿ƒåŸç†ä¸ä¼ ç»Ÿç»å…¸è®¡...\n",
      "\n",
      "ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰:\n",
      "- è€—æ—¶: 0.00ç§’\n",
      "- ç»“æœå‰50ä¸ªå­—ç¬¦: é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦åŸç†ï¼ˆå¦‚å åŠ æ€å’Œçº ç¼ æ€ï¼‰è¿›è¡Œä¿¡æ¯å¤„ç†çš„æ–°å‹è®¡ç®—èŒƒå¼ã€‚å…¶æ ¸å¿ƒåŸç†ä¸ä¼ ç»Ÿç»å…¸è®¡...\n",
      "\n",
      "æ€§èƒ½æå‡: 84330.99å€\n",
      "ç¼“å­˜èŠ‚çœäº† 45.26 ç§’\n"
     ]
    }
   ],
   "source": [
    "# æ¼”ç¤ºç¼“å­˜ä¼˜åŒ–æ•ˆæœ\n",
    "import time\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æµ‹é‡æ‰§è¡Œæ—¶é—´\n",
    "def measure_time(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰\n",
    "print(\"===== ç¼“å­˜ä¼˜åŒ–æ¼”ç¤º =====\")\n",
    "prompt = \"è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†\"\n",
    "result1, time1 = measure_time(llm.invoke, prompt)\n",
    "print(f\"ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰:\")\n",
    "print(f\"- è€—æ—¶: {time1:.2f}ç§’\")\n",
    "print(f\"- ç»“æœå‰50ä¸ªå­—ç¬¦: {result1.content[:50]}...\\n\")\n",
    "\n",
    "# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰\n",
    "result2, time2 = measure_time(llm.invoke, prompt)\n",
    "print(f\"ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰:\")\n",
    "print(f\"- è€—æ—¶: {time2:.2f}ç§’\")\n",
    "print(f\"- ç»“æœå‰50ä¸ªå­—ç¬¦: {result2.content[:50]}...\\n\")\n",
    "\n",
    "# æ˜¾ç¤ºæ€§èƒ½æå‡\n",
    "if time1 > 0:\n",
    "    speedup = time1 / time2 if time2 > 0 else float('inf')\n",
    "    print(f\"æ€§èƒ½æå‡: {speedup:.2f}å€\")\n",
    "    print(f\"ç¼“å­˜èŠ‚çœäº† {(time1 - time2):.2f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç”¨æˆ·ä½“éªŒä¼˜åŒ– "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 é”™è¯¯å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! ğŸ˜Š How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.exceptions import LangChainException \n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "try:\n",
    "    llm = ChatDeepSeek(model='deepseek-chat')\n",
    "    response = await llm.ainvoke(\"Hello\")\n",
    "    print(response.content)\n",
    "except LangChainException as e:\n",
    "    # å¤„ç†é”™è¯¯\n",
    "    error_message = f\"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\"\n",
    "    # è¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯\n",
    "    print(error_message)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 è¿›åº¦åé¦ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ç›´æ¥åˆ›å»ºå›è°ƒå¤„ç†å™¨\n",
    "callback_handler = StreamingStdOutCallbackHandler()\n",
    "\n",
    "# ä½¿ç”¨å›è°ƒ\n",
    "llm = ChatDeepSeek(model='deepseek-chat', callbacks=[callback_handler])\n",
    "\n",
    "# åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_template(\"è¯·å‘Šè¯‰æˆ‘å…³äº{topic}çš„ä¿¡æ¯\")\n",
    "\n",
    "# åˆ›å»ºé“¾å¹¶ä½¿ç”¨å›è°ƒ\n",
    "# åœ¨LangChainä¸­ï¼Œä½¿ç”¨|è¿ç®—ç¬¦è¿æ¥æ—¶ï¼Œéœ€è¦ç¡®ä¿ä¸¤è¾¹éƒ½æ˜¯Runnableå¯¹è±¡\n",
    "chain = {\"topic\": lambda x: x} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "å°ç±³ï¼ˆXiaomiï¼‰æ˜¯ä¸€å®¶ä¸­å›½çŸ¥åçš„ç§‘æŠ€å…¬å¸ï¼Œæˆç«‹äº2010å¹´ï¼Œæ€»éƒ¨ä½äºåŒ—äº¬ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºå°ç±³çš„å…³é”®ä¿¡æ¯ï¼š\n",
       "\n",
       "### 1. **å…¬å¸ç®€ä»‹**\n",
       "   - **æˆç«‹æ—¶é—´**ï¼š2010å¹´4æœˆ6æ—¥ã€‚\n",
       "   - **åˆ›å§‹äºº**ï¼šé›·å†›ï¼ˆLei Junï¼‰ã€‚\n",
       "   - **æ€»éƒ¨**ï¼šä¸­å›½åŒ—äº¬ã€‚\n",
       "   - **ä¸šåŠ¡èŒƒå›´**ï¼šæ™ºèƒ½æ‰‹æœºã€æ™ºèƒ½å®¶å±…ã€æ¶ˆè´¹ç”µå­äº§å“ã€äº’è”ç½‘æœåŠ¡ç­‰ã€‚\n",
       "\n",
       "### 2. **ä¸»è¦äº§å“**\n",
       "   - **æ™ºèƒ½æ‰‹æœº**ï¼šå°ç±³çš„æ™ºèƒ½æ‰‹æœºæ˜¯å…¶æ ¸å¿ƒäº§å“ï¼Œå¦‚å°ç±³ç³»åˆ—ã€Redmiç³»åˆ—ã€POCOç³»åˆ—ç­‰ã€‚\n",
       "   - **æ™ºèƒ½å®¶å±…**ï¼šå°ç±³ç”Ÿæ€é“¾åŒ…æ‹¬æ™ºèƒ½ç”µè§†ã€ç©ºæ°”å‡€åŒ–å™¨ã€æ‰«åœ°æœºå™¨äººã€æ™ºèƒ½éŸ³ç®±ç­‰ã€‚\n",
       "   - **å…¶ä»–äº§å“**ï¼šç¬”è®°æœ¬ç”µè„‘ã€ç©¿æˆ´è®¾å¤‡ï¼ˆå¦‚æ‰‹ç¯ã€æ‰‹è¡¨ï¼‰ã€è·¯ç”±å™¨ç­‰ã€‚\n",
       "\n",
       "### 3. **å•†ä¸šæ¨¡å¼**\n",
       "   - **é«˜æ€§ä»·æ¯”**ï¼šå°ç±³ä»¥â€œæ€§ä»·æ¯”â€è‘—ç§°ï¼Œé€šè¿‡çº¿ä¸Šé”€å”®å’Œé«˜æ•ˆçš„ä¾›åº”é“¾ç®¡ç†é™ä½æˆæœ¬ã€‚\n",
       "   - **ç”Ÿæ€é“¾æˆ˜ç•¥**ï¼šæŠ•èµ„å’Œæ”¯æŒå¤šå®¶ç”Ÿæ€é“¾ä¼ä¸šï¼Œæ‰©å±•äº§å“çº¿ã€‚\n",
       "\n",
       "### 4. **å¸‚åœºè¡¨ç°**\n",
       "   - **å…¨çƒå¸‚åœº**ï¼šå°ç±³æ˜¯å…¨çƒå‰ä¸‰çš„æ™ºèƒ½æ‰‹æœºåˆ¶é€ å•†ä¹‹ä¸€ï¼Œåœ¨å°åº¦ã€æ¬§æ´²ç­‰å¸‚åœºè¡¨ç°å¼ºåŠ²ã€‚\n",
       "   - **ä¸Šå¸‚**ï¼š2018å¹´7æœˆåœ¨é¦™æ¸¯äº¤æ˜“æ‰€ä¸Šå¸‚ï¼ˆè‚¡ç¥¨ä»£ç ï¼š1810.HKï¼‰ã€‚\n",
       "\n",
       "### 5. **æŠ€æœ¯åˆ›æ–°**\n",
       "   - **MIUI**ï¼šåŸºäºAndroidçš„å®šåˆ¶æ“ä½œç³»ç»Ÿã€‚\n",
       "   - **ç ”å‘æŠ•å…¥**ï¼šåœ¨ç›¸æœºæŠ€æœ¯ã€å¿«å……ã€AIç­‰é¢†åŸŸæœ‰æ˜¾è‘—è¿›å±•ã€‚\n",
       "\n",
       "### 6. **æœ€æ–°åŠ¨æ€**\n",
       "   - **ç”µåŠ¨æ±½è½¦**ï¼š2024å¹´æ¨å‡ºé¦–æ¬¾ç”µåŠ¨æ±½è½¦SU7ï¼Œè¿›å†›æ±½è½¦è¡Œä¸šã€‚\n",
       "   - **å…¨çƒåŒ–**ï¼šæŒç»­æ‰©å±•å›½é™…å¸‚åœºï¼Œå°¤å…¶åœ¨ä¸œå—äºšã€æ‹‰ç¾ç­‰åœ°ã€‚\n",
       "\n",
       "### 7. **ä¼ä¸šæ–‡åŒ–**\n",
       "   - **ä½¿å‘½**ï¼šâ€œè®©å…¨çƒæ¯ä¸ªäººéƒ½èƒ½äº«å—ç§‘æŠ€å¸¦æ¥çš„ç¾å¥½ç”Ÿæ´»â€ã€‚\n",
       "   - **ä»·å€¼è§‚**ï¼šç”¨æˆ·å¯¼å‘ã€åˆ›æ–°ã€è´¨é‡ã€‚\n",
       "\n",
       "å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼ˆå¦‚è´¢åŠ¡æ•°æ®ã€å…·ä½“äº§å“å‚æ•°ç­‰ï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé—®ï¼"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chain.invoke(\"å°ç±³\").content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

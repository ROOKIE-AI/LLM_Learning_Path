# LangChain 实践案例

## 1. 开发流程

### 1.1 项目规划

#### 1.1.1 需求分析

```python
# 需求分析示例
requirements = {
    "功能需求": [
        "文本生成",
        "对话交互",
        "知识检索",
        "数据分析"
    ],
    "性能需求": [
        "响应时间 < 2秒",
        "并发用户 > 100",
        "准确率 > 90%"
    ],
    "安全需求": [
        "用户认证",
        "数据加密",
        "访问控制"
    ]
}
```

#### 1.1.2 架构设计

```python
# 系统架构示例
architecture = {
    "前端层": {
        "Web界面": "React/Vue",
        "移动端": "Flutter/React Native"
    },
    "应用层": {
        "API服务": "FastAPI/Flask",
        "业务逻辑": "LangChain组件"
    },
    "数据层": {
        "向量数据库": "Chroma/Pinecone",
        "关系数据库": "PostgreSQL/MySQL"
    }
}
```

### 1.2 实现步骤

#### 1.2.1 环境搭建

```bash
# 创建项目目录
mkdir langchain_project
cd langchain_project

# 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# 安装依赖
pip install -r requirements.txt
```

#### 1.2.2 代码开发

```python
# 项目结构
project_structure = {
    "src/": {
        "config/": "配置文件",
        "models/": "数据模型",
        "services/": "业务服务",
        "utils/": "工具函数",
        "api/": "API接口"
    },
    "tests/": "测试用例",
    "docs/": "文档",
    "requirements.txt": "依赖列表"
}
```

## 2. 最佳实践

### 2.1 开发规范

#### 2.1.1 代码规范

```python
# 代码规范示例
class LangChainService:
    """LangChain服务类
    
    用于处理LangChain相关的业务逻辑
    """
    
    def __init__(self, model_name: str, temperature: float = 0.7):
        """初始化服务
        
        Args:
            model_name: 模型名称
            temperature: 温度参数
        """
        self.llm = OpenAI(
            model_name=model_name,
            temperature=temperature
        )
    
    def generate_text(self, prompt: str) -> str:
        """生成文本
        
        Args:
            prompt: 输入提示词
            
        Returns:
            生成的文本
        """
        try:
            response = self.llm.generate([prompt])
            return response.generations[0][0].text
        except Exception as e:
            logger.error(f"生成文本失败：{str(e)}")
            raise
```

#### 2.1.2 测试规范

```python
# 测试用例示例
import pytest
from langchain_service import LangChainService

def test_text_generation():
    service = LangChainService("gpt-3.5-turbo")
    prompt = "你好"
    response = service.generate_text(prompt)
    assert len(response) > 0
    assert isinstance(response, str)

def test_error_handling():
    service = LangChainService("invalid-model")
    with pytest.raises(Exception):
        service.generate_text("test")
```

### 2.2 性能优化

#### 2.2.1 响应优化

```python
# 异步处理示例
from fastapi import FastAPI, BackgroundTasks
from langchain_service import LangChainService

app = FastAPI()
service = LangChainService("gpt-3.5-turbo")

@app.post("/generate")
async def generate_text(
    prompt: str,
    background_tasks: BackgroundTasks
):
    # 异步处理
    background_tasks.add_task(
        service.generate_text,
        prompt
    )
    return {"status": "processing"}

# 缓存优化
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_generate(prompt: str) -> str:
    return service.generate_text(prompt)
```

#### 2.2.2 资源优化

```python
# 连接池示例
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

class VectorStoreManager:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
    
    def get_vectorstore(self):
        if self.vectorstore is None:
            self.vectorstore = Chroma(
                collection_name="documents",
                embedding_function=self.embeddings
            )
        return self.vectorstore

# 内存管理
import gc

def cleanup_resources():
    gc.collect()  # 垃圾回收
    if torch.cuda.is_available():
        torch.cuda.empty_cache()  # 清理GPU内存
```

## 3. 实践案例

### 3.1 智能客服系统

#### 3.1.1 系统架构

```python
class CustomerServiceSystem:
    def __init__(self):
        self.llm = ChatOpenAI(temperature=0.7)
        self.memory = ConversationBufferMemory()
        self.vectorstore = Chroma(
            collection_name="faq",
            embedding_function=OpenAIEmbeddings()
        )
    
    def process_query(self, query: str) -> str:
        # 1. 检索相关FAQ
        relevant_docs = self.vectorstore.similarity_search(query)
        
        # 2. 生成上下文
        context = self._generate_context(relevant_docs)
        
        # 3. 生成回答
        response = self._generate_response(query, context)
        
        # 4. 更新对话历史
        self.memory.save_context(
            {"input": query},
            {"output": response}
        )
        
        return response
```

#### 3.1.2 实现细节

```python
# FAQ管理
class FAQManager:
    def __init__(self):
        self.vectorstore = Chroma(
            collection_name="faq",
            embedding_function=OpenAIEmbeddings()
        )
    
    def add_faq(self, question: str, answer: str):
        self.vectorstore.add_texts(
            texts=[f"Q: {question}\nA: {answer}"]
        )
    
    def search_faq(self, query: str) -> List[str]:
        return self.vectorstore.similarity_search(query)

# 情感分析
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

class SentimentAnalyzer:
    def __init__(self):
        self.prompt = PromptTemplate(
            input_variables=["text"],
            template="分析以下文本的情感倾向：{text}"
        )
        self.chain = LLMChain(
            llm=OpenAI(temperature=0),
            prompt=self.prompt
        )
    
    def analyze(self, text: str) -> str:
        return self.chain.run(text)
```

### 3.2 知识库问答系统

#### 3.2.1 系统实现

```python
class KnowledgeBaseQA:
    def __init__(self):
        self.llm = OpenAI(temperature=0.7)
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
    
    def load_documents(self, documents: List[Document]):
        # 1. 分割文档
        text_splitter = CharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        # 2. 创建向量存储
        self.vectorstore = Chroma.from_documents(
            documents=texts,
            embedding=self.embeddings
        )
    
    def answer_question(self, question: str) -> str:
        # 1. 检索相关文档
        relevant_docs = self.vectorstore.similarity_search(question)
        
        # 2. 生成答案
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever()
        )
        
        return qa_chain.run(question)
```

#### 3.2.2 优化策略

```python
# 文档预处理
def preprocess_document(doc: Document) -> Document:
    # 1. 清理文本
    text = clean_text(doc.page_content)
    
    # 2. 提取关键信息
    metadata = extract_metadata(doc.metadata)
    
    # 3. 分段处理
    chunks = split_into_chunks(text)
    
    return Document(
        page_content=text,
        metadata=metadata
    )

# 答案优化
def optimize_answer(answer: str) -> str:
    # 1. 格式规范化
    answer = format_answer(answer)
    
    # 2. 内容优化
    answer = enhance_content(answer)
    
    # 3. 引用来源
    answer = add_references(answer)
    
    return answer
```

## 4. 部署方案

### 4.1 容器化部署

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 4.2 服务编排

```yaml
# docker-compose.yml
version: '3'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
      
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
```

## 5. 监控与维护

### 5.1 性能监控

```python
# 性能监控示例
from prometheus_client import Counter, Histogram

# 定义指标
request_counter = Counter(
    'langchain_requests_total',
    'Total number of requests'
)

response_time = Histogram(
    'langchain_response_time_seconds',
    'Response time in seconds'
)

# 记录指标
@response_time.time()
def process_request():
    request_counter.inc()
    return service.generate_text(prompt)
```

### 5.2 日志管理

```python
# 日志配置
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# 记录日志
logger.info("开始处理请求")
try:
    response = service.process_request()
    logger.info("请求处理成功")
except Exception as e:
    logger.error(f"请求处理失败：{str(e)}")
```
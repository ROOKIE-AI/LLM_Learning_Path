{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记本涵盖PyTorch深度学习框架的核心概念和实践应用，从基础到高级特性，特别关注大模型开发相关技术。\n",
    "\n",
    "## 目录\n",
    "1. **PyTorch基础**\n",
    "   - 张量(Tensor)基础操作\n",
    "   - 自动微分(Autograd)\n",
    "   - 神经网络模块(nn.Module)\n",
    "   - 优化器(Optimizer)\n",
    "   - 数据加载与处理(DataLoader)\n",
    "\n",
    "2. **模型构建与训练**\n",
    "   - 线性模型与多层感知机\n",
    "   - 卷积神经网络(CNN)\n",
    "   - 循环神经网络(RNN)\n",
    "   - Transformer架构\n",
    "   - 损失函数与评估指标\n",
    "\n",
    "3. **高级特性**\n",
    "   - 分布式训练\n",
    "   - 混合精度训练\n",
    "   - 模型量化与优化\n",
    "   - TorchScript与模型部署\n",
    "   - CUDA编程与GPU加速\n",
    "\n",
    "4. **大模型应用**\n",
    "   - 预训练模型加载与使用\n",
    "   - 微调(Fine-tuning)技术\n",
    "   - 模型并行与流水线并行\n",
    "   - 梯度检查点(Gradient Checkpointing)\n",
    "   - 模型蒸馏与压缩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch基础\n",
    "- 张量(Tensor)基础操作\n",
    "- 自动微分(Autograd)\n",
    "- 神经网络模块(nn.Module)\n",
    "- 优化器(Optimizer)\n",
    "- 数据加载与处理(DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 张量的基础操作\n",
    "\n",
    "张量(Tensor)是PyTorch中的核心数据结构，类似于NumPy的多维数组，但可以在GPU上运行并支持自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建张量\n",
    "- 从Python列表或NumPy数组创建\n",
    "- 使用预定义函数(zeros, ones, rand等)创建\n",
    "- 指定设备(CPU/GPU)和数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从列表创建的张量:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "从NumPy数组创建的张量:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "全1张量:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "全0张量:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "随机张量(均匀分布):\n",
      "tensor([[0.7765, 0.1617, 0.3345],\n",
      "        [0.3911, 0.3256, 0.9005]])\n",
      "随机张量(正态分布):\n",
      "tensor([[-0.5779,  1.6642, -0.6261],\n",
      "        [ 0.5650,  1.7616,  1.0969]])\n",
      "float类型张量:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "long类型张量:\n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "当前环境没有可用的GPU\n"
     ]
    }
   ],
   "source": [
    "# 从Python列表创建张量\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"从列表创建的张量:\\n{x_data}\")\n",
    "\n",
    "# 从NumPy数组创建张量\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"从NumPy数组创建的张量:\\n{x_np}\")\n",
    "\n",
    "# 使用预定义函数创建张量\n",
    "x_ones = torch.ones(2, 3)  # 全1张量\n",
    "x_zeros = torch.zeros(2, 3)  # 全0张量\n",
    "x_rand = torch.rand(2, 3)  # 随机张量(0-1均匀分布)\n",
    "x_randn = torch.randn(2, 3)  # 随机张量(标准正态分布)\n",
    "\n",
    "print(f\"全1张量:\\n{x_ones}\")\n",
    "print(f\"全0张量:\\n{x_zeros}\")\n",
    "print(f\"随机张量(均匀分布):\\n{x_rand}\")\n",
    "print(f\"随机张量(正态分布):\\n{x_randn}\")\n",
    "\n",
    "# 指定数据类型\n",
    "x_float = torch.zeros(2, 2, dtype=torch.float32)\n",
    "x_long = torch.zeros(2, 2, dtype=torch.int64)\n",
    "print(f\"float类型张量:\\n{x_float}\")\n",
    "print(f\"long类型张量:\\n{x_long}\")\n",
    "\n",
    "# 指定设备\n",
    "# 检查是否有可用的GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x_gpu = torch.rand(2, 2, device=device)  # 直接在GPU上创建\n",
    "    x_cpu = torch.rand(2, 2)  # 在CPU上创建\n",
    "    x_gpu2 = x_cpu.to(device)  # CPU张量转移到GPU\n",
    "    print(f\"GPU张量设备: {x_gpu.device}\")\n",
    "    print(f\"CPU张量转GPU后设备: {x_gpu2.device}\")\n",
    "else:\n",
    "    print(\"当前环境没有可用的GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本操作\n",
    "- 算术运算(加减乘除)\n",
    "- 索引与切片\n",
    "- 形状操作(reshape, view, squeeze等)\n",
    "- 数学函数(sin, exp, log等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加法: tensor([5, 7, 9])\n",
      "加法函数: tensor([5, 7, 9])\n",
      "减法: tensor([3, 3, 3])\n",
      "减法函数: tensor([3, 3, 3])\n",
      "乘法: tensor([ 4, 10, 18])\n",
      "乘法函数: tensor([ 4, 10, 18])\n",
      "除法: tensor([4.0000, 2.5000, 2.0000])\n",
      "除法函数: tensor([4.0000, 2.5000, 2.0000])\n",
      "矩阵乘法:\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "矩阵乘法(另一种写法):\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "原始张量:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "第一行: tensor([1, 2, 3])\n",
      "第一列: tensor([1, 4, 7])\n",
      "子矩阵:\n",
      "tensor([[2, 3],\n",
      "        [5, 6]])\n",
      "原始张量形状: torch.Size([4, 4])\n",
      "view后形状: torch.Size([16])\n",
      "reshape后形状: torch.Size([2, 8])\n",
      "unsqueeze后形状: torch.Size([1, 4, 4])\n",
      "squeeze后形状: torch.Size([4, 4])\n",
      "转置后形状: torch.Size([4, 4])\n",
      "x: tensor([0.0000, 1.5708, 3.1416])\n",
      "sin(x): tensor([ 0.0000e+00,  1.0000e+00, -8.7423e-08])\n",
      "cos(x): tensor([ 1.0000e+00, -4.3711e-08, -1.0000e+00])\n",
      "exp(x): tensor([ 1.0000,  4.8105, 23.1407])\n",
      "log(exp(x)): tensor([0.0000, 1.5708, 3.1416])\n",
      "x:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "求和: 21.0\n",
      "按行求和: tensor([ 6., 15.])\n",
      "按列求和: tensor([5., 7., 9.])\n",
      "最大值: 6.0\n",
      "最小值: 1.0\n",
      "平均值: 3.5\n"
     ]
    }
   ],
   "source": [
    "# 算术运算\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 加法\n",
    "print(f\"加法: {a + b}\")\n",
    "print(f\"加法函数: {torch.add(a, b)}\")\n",
    "\n",
    "# 减法\n",
    "print(f\"减法: {b - a}\")\n",
    "print(f\"减法函数: {torch.sub(b, a)}\")\n",
    "\n",
    "# 乘法\n",
    "print(f\"乘法: {a * b}\")\n",
    "print(f\"乘法函数: {torch.mul(a, b)}\")\n",
    "\n",
    "# 除法\n",
    "print(f\"除法: {b / a}\")\n",
    "print(f\"除法函数: {torch.div(b, a)}\")\n",
    "\n",
    "# 矩阵乘法\n",
    "m1 = torch.tensor([[1, 2], [3, 4]])\n",
    "m2 = torch.tensor([[5, 6], [7, 8]])\n",
    "print(f\"矩阵乘法:\\n{torch.matmul(m1, m2)}\")\n",
    "print(f\"矩阵乘法(另一种写法):\\n{m1 @ m2}\")\n",
    "\n",
    "# 索引与切片\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(f\"原始张量:\\n{x}\")\n",
    "print(f\"第一行: {x[0]}\")\n",
    "print(f\"第一列: {x[:, 0]}\")\n",
    "print(f\"子矩阵:\\n{x[0:2, 1:3]}\")\n",
    "\n",
    "# 形状操作\n",
    "x = torch.randn(4, 4)\n",
    "print(f\"原始张量形状: {x.shape}\")\n",
    "\n",
    "# 改变形状\n",
    "y = x.view(16)  # 展平为一维\n",
    "print(f\"view后形状: {y.shape}\")\n",
    "\n",
    "# reshape操作\n",
    "z = x.reshape(2, 8)  # 重塑为2x8\n",
    "print(f\"reshape后形状: {z.shape}\")\n",
    "\n",
    "# 添加维度\n",
    "x_unsqueezed = x.unsqueeze(0)  # 在第0维添加维度\n",
    "print(f\"unsqueeze后形状: {x_unsqueezed.shape}\")\n",
    "\n",
    "# 删除维度\n",
    "x_squeezed = x_unsqueezed.squeeze(0)  # 删除第0维(如果是1)\n",
    "print(f\"squeeze后形状: {x_squeezed.shape}\")\n",
    "\n",
    "# 转置\n",
    "x_t = x.t()\n",
    "print(f\"转置后形状: {x_t.shape}\")\n",
    "\n",
    "# 数学函数\n",
    "x = torch.tensor([0.0, math.pi/2, math.pi])\n",
    "print(f\"x: {x}\")\n",
    "print(f\"sin(x): {torch.sin(x)}\")\n",
    "print(f\"cos(x): {torch.cos(x)}\")\n",
    "print(f\"exp(x): {torch.exp(x)}\")\n",
    "print(f\"log(exp(x)): {torch.log(torch.exp(x))}\")\n",
    "\n",
    "# 聚合函数\n",
    "x = torch.tensor([[1.0, 2, 3], [4, 5, 6]])\n",
    "print(f\"x:\\n{x}\")\n",
    "print(f\"求和: {x.sum()}\")\n",
    "print(f\"按行求和: {x.sum(dim=1)}\")\n",
    "print(f\"按列求和: {x.sum(dim=0)}\")\n",
    "print(f\"最大值: {x.max()}\")\n",
    "print(f\"最小值: {x.min()}\")\n",
    "print(f\"平均值: {x.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 广播机制\n",
    "- 不同形状张量间的自动扩展规则\n",
    "- 提高计算效率和代码简洁性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x形状: torch.Size([3])\n",
      "y形状: torch.Size([2, 1])\n",
      "广播后z形状: torch.Size([2, 3])\n",
      "z:\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5]])\n",
      "a * b: tensor([ 5, 10, 15])\n",
      "matrix + vector:\n",
      "tensor([[11, 22],\n",
      "        [13, 24]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制示例\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([[1], [2]])\n",
    "\n",
    "# 广播相加\n",
    "z = x + y\n",
    "print(f\"x形状: {x.shape}\")\n",
    "print(f\"y形状: {y.shape}\")\n",
    "print(f\"广播后z形状: {z.shape}\")\n",
    "print(f\"z:\\n{z}\")\n",
    "\n",
    "# 广播乘法\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor(5)\n",
    "print(f\"a * b: {a * b}\")\n",
    "\n",
    "# 广播矩阵运算\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "vector = torch.tensor([10, 20])\n",
    "print(f\"matrix + vector:\\n{matrix + vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设备间移动\n",
    "- CPU与GPU之间的数据传输\n",
    "- 使用.to()方法指定设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: cpu\n",
      "原始张量设备: cpu\n",
      "移动后张量设备: cpu\n",
      "计算结果设备: cpu\n",
      "移回CPU后设备: cpu\n"
     ]
    }
   ],
   "source": [
    "# 检查CUDA是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前设备: {device}\")\n",
    "\n",
    "# 创建张量并移动到指定设备\n",
    "x = torch.randn(3, 4)\n",
    "print(f\"原始张量设备: {x.device}\")\n",
    "\n",
    "# 移动到GPU (如果可用)\n",
    "x_device = x.to(device)\n",
    "print(f\"移动后张量设备: {x_device.device}\")\n",
    "\n",
    "# 在设备上进行计算\n",
    "y_device = x_device * 2\n",
    "print(f\"计算结果设备: {y_device.device}\")\n",
    "\n",
    "# 如需要，可以将结果移回CPU\n",
    "y_cpu = y_device.to(\"cpu\")\n",
    "print(f\"移回CPU后设备: {y_cpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与NumPy的互操作性\n",
    "- tensor与ndarray的相互转换\n",
    "- 共享内存优化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch张量:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "NumPy数组:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "NumPy数组:\n",
      "[[0.1557848  0.41081753 0.67040709]\n",
      " [0.77537471 0.97119819 0.54644364]]\n",
      "PyTorch张量:\n",
      "tensor([[0.1558, 0.4108, 0.6704],\n",
      "        [0.7754, 0.9712, 0.5464]], dtype=torch.float64)\n",
      "修改后NumPy数组:\n",
      "[[100.           0.41081753   0.67040709]\n",
      " [  0.77537471   0.97119819   0.54644364]]\n",
      "对应的PyTorch张量:\n",
      "tensor([[100.0000,   0.4108,   0.6704],\n",
      "        [  0.7754,   0.9712,   0.5464]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch张量转NumPy数组\n",
    "tensor = torch.ones(3, 4)\n",
    "numpy_array = tensor.numpy()\n",
    "print(f\"PyTorch张量:\\n{tensor}\")\n",
    "print(f\"NumPy数组:\\n{numpy_array}\")\n",
    "\n",
    "# NumPy数组转PyTorch张量\n",
    "numpy_array = np.random.rand(2, 3)\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"NumPy数组:\\n{numpy_array}\")\n",
    "print(f\"PyTorch张量:\\n{tensor_from_numpy}\")\n",
    "\n",
    "# 注意：共享内存，修改一个会影响另一个\n",
    "numpy_array[0, 0] = 100\n",
    "print(f\"修改后NumPy数组:\\n{numpy_array}\")\n",
    "print(f\"对应的PyTorch张量:\\n{tensor_from_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch的自动微分系统是深度学习框架的核心功能，它能够自动计算神经网络中的梯度。\n",
    "\n",
    "#### 主要特点\n",
    "- 动态计算图：PyTorch使用动态计算图，可以在运行时改变网络结构\n",
    "- 反向传播：自动计算梯度，用于模型参数更新\n",
    "- 高效计算：针对GPU优化的梯度计算\n",
    "\n",
    "#### 基本用法\n",
    "- `requires_grad=True`：标记需要计算梯度的张量\n",
    "- `.backward()`：执行反向传播计算梯度\n",
    "- `.grad`：访问计算得到的梯度\n",
    "- `with torch.no_grad()`：临时禁用梯度计算\n",
    "- `torch.autograd.grad()`：直接计算梯度\n",
    "\n",
    "#### 高级功能\n",
    "- 高阶导数计算\n",
    "- 自定义自动微分函数\n",
    "- 梯度检查与调试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型构建与训练\n",
    "- 线性模型与多层感知机\n",
    "- 卷积神经网络(CNN)\n",
    "- 循环神经网络(RNN)\n",
    "- Transformer架构\n",
    "- 损失函数与评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 高级特性\n",
    "- 分布式训练\n",
    "- 混合精度训练\n",
    "- 模型量化与优化\n",
    "- TorchScript与模型部署\n",
    "- CUDA编程与GPU加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 大模型应用\n",
    "- 预训练模型加载与使用\n",
    "- 微调(Fine-tuning)技术\n",
    "- 模型并行与流水线并行\n",
    "- 梯度检查点(Gradient Checkpointing)\n",
    "- 模型蒸馏与压缩\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
